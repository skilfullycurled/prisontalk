{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## RESOURCES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### PACKAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-28T09:32:13.407822Z",
     "start_time": "2019-01-28T09:32:13.379744Z"
    },
    "code_folding": [],
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# DATA\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# READING/WRITING\n",
    "import json\n",
    "import csv\n",
    "\n",
    "# PARSING\n",
    "from bs4 import BeautifulSoup, NavigableString, Tag\n",
    "import lxml.html\n",
    "import requests\n",
    "from string import capwords\n",
    "from titlecase import titlecase\n",
    "\n",
    "# DATES/TIMES\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "from pytz import timezone\n",
    "import time\n",
    "\n",
    "# SYSTEM\n",
    "import os, sys\n",
    "from pathlib2 import Path\n",
    "import ConfigParser\n",
    "import multiprocessing as mp\n",
    "\n",
    "# ERROR HANDLING\n",
    "import errno\n",
    "import traceback\n",
    "\n",
    "# LOGGING\n",
    "import logging\n",
    "import smtplib\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.text import MIMEText\n",
    "\n",
    "# MISC\n",
    "import collections\n",
    "import re\n",
    "import copy\n",
    "import random\n",
    "from pprint import pprint\n",
    "import uuid\n",
    "\n",
    "\n",
    "\n",
    "# print 'ALL MODULES LOADED'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-27T21:44:10.524747Z",
     "start_time": "2019-01-27T21:44:09.584502Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# fid_110_tid_706047_pg_1.html - ValueError: time data 'Today 12:35 PM' does not match format '%m-%d-%Y %I:%M %p'\n",
    "    \n",
    "test_path = '../data/samples/'\n",
    "test_fid = '110/'\n",
    "test_file = 'fid_110_tid_706047_pg_1.html'\n",
    "\n",
    "html_page = load_html_file(test_path + test_fid + test_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## FUNCTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### POSTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### GET HTML POSTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-28T09:32:13.417333Z",
     "start_time": "2019-01-28T09:32:13.410522Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def get_html_posts(html_page):\n",
    "    \n",
    "    post_tables = html_page.find_all('table', {'id': re.compile('post')})\n",
    "    post_rows = [post_table.find_all('tr', recursive=False) for post_table in post_tables]\n",
    "    \n",
    "    return post_rows\n",
    "    \n",
    "# html_posts = get_html_posts(html_page)\n",
    "# len(html_posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### GET POST DATE AND TIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-28T09:32:13.434921Z",
     "start_time": "2019-01-28T09:32:13.419755Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def get_date_time(el):\n",
    "    \n",
    "    top_bar = el.find_all('td', {'class': 'thead'})[0]\n",
    "    top_bar_text = top_bar.get_text().encode('ascii', 'ignore').decode('ascii')\n",
    "    top_bar_text = top_bar_text.replace('#', '').replace(',', '')\n",
    "    \n",
    "    if ('Today' or 'Yesterday') in top_bar_text:\n",
    "        \n",
    "        date_time = top_bar_text\n",
    "            \n",
    "    else:\n",
    "\n",
    "        local_datetime = datetime.strptime(top_bar_text.split(' ', 1)[1], '%m-%d-%Y %I:%M %p')\n",
    "        date_time = convert_to_utc(local_datetime, 'US/Mountain')\n",
    "    \n",
    "#     return utc_datetime\n",
    "    return date_time\n",
    "\n",
    "# get_date_time(html_posts[0][0]) # '2005-01-07 11:17:00 UTC+0000'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### GET POST ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-28T09:32:13.443013Z",
     "start_time": "2019-01-28T09:32:13.437327Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def get_post_id(el):\n",
    "    \n",
    "    post_id_link = el.find_all('a', {'id': re.compile('postcount')})\n",
    "    pid = post_id_link[0]['id'].split('t')[-1]\n",
    "    \n",
    "    return pid\n",
    "\n",
    "# get_post_id(html_posts[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### GET POST NUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-28T09:32:13.453723Z",
     "start_time": "2019-01-28T09:32:13.445480Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def get_post_num(el):\n",
    "    \n",
    "    top_bar = el.find_all('td', {'class': 'thead'})[0]\n",
    "    top_bar_text = top_bar.get_text().encode('ascii', 'ignore').decode('ascii')\n",
    "    top_bar_text = top_bar_text.replace('#', '').replace(',', '')\n",
    "    \n",
    "    post_count = top_bar_text.split(' ', 1)[0]\n",
    "    \n",
    "    return post_count\n",
    "\n",
    "# get_post_num(html_posts[0][0]) #41"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### GET POST TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-28T09:32:13.466350Z",
     "start_time": "2019-01-28T09:32:13.456291Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def get_post_text(el):\n",
    "    \n",
    "    post_msg_el = el.find_all('div', {'id': re.compile('post_message')})[0]\n",
    "    \n",
    "    try: \n",
    "        post_msg_el.div.decompose() # TO REMOVE QUOTE\n",
    "\n",
    "    except AttributeError:\n",
    "\n",
    "        pass # LET'S EVENTUALLY LOG SOMETHING HERE\n",
    "\n",
    "    msg_text = post_msg_el.get_text(\" \", strip=True)\n",
    "    \n",
    "    return msg_text\n",
    "\n",
    "# reg_trs = posts[0].find_all('tr', recursive=False)\n",
    "# quote_trs = posts[8].find_all('tr', recursive=False)\n",
    "# print 'NO QUOTE', '\\n', get_post_message(reg_trs[2]), '\\n' # \"Met a boy, because if he...with a womanÂ’s heart.\"\n",
    "# print 'WITH QUOTE', '\\n', get_post_message(quote_trs[2]) # \"I am glad this information...which must be horrible\"\n",
    "# get_post_text(html_posts[0][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### GET FORUM INFO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-28T09:32:13.480646Z",
     "start_time": "2019-01-28T09:32:13.468906Z"
    },
    "code_folding": [],
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def get_forum_id(file_path):\n",
    "    \n",
    "    file_name = file_path.split('/')[-1]\n",
    "    \n",
    "    return file_name.split('_')[1]\n",
    "\n",
    "def get_thread_id(file_path):\n",
    "    \n",
    "    file_name = file_path.split('/')[-1]\n",
    "    \n",
    "    return file_name.split('_')[3]\n",
    "\n",
    "def get_page_number(file_path):\n",
    "    \n",
    "    file_name = file_path.split('/')[-1]\n",
    "    \n",
    "    return file_name.split('_')[-1].split('.')[0]\n",
    "\n",
    "# file_path = '../examples/threads/parsing/1506/fid_1506_tid_97875_pg_4.html'\n",
    "# print 'FORUM:', get_forum_id(file_path), 'THREAD ID:', get_thread_id(file_path), 'PAGE:', get_page_number(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### GET POST INFO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-28T09:32:13.501985Z",
     "start_time": "2019-01-28T09:32:13.483166Z"
    },
    "code_folding": [],
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def get_post_info(file_name, post_el):\n",
    "    \n",
    "    post = collections.OrderedDict()\n",
    "    \n",
    "    post['date'] = get_date_time(post_el[0])\n",
    "    \n",
    "    post['fid'] = get_forum_id(file_name)\n",
    "    post['tid'] = get_thread_id(file_name)\n",
    "    post['pg'] = get_page_number(file_name)\n",
    "    \n",
    "    post['pid'] = get_post_id(post_el[0])\n",
    "    post['post_num'] = get_post_num(post_el[0])\n",
    "    \n",
    "    post['user'] = get_user_name(post_el[1])\n",
    "    post['uid'] = get_user_id(post_el[1])\n",
    "\n",
    "    post['text'] = get_post_text(post_el[2])\n",
    "    \n",
    "    return post\n",
    "\n",
    "# dir_path = '../examples/threads/parsing/1506/'\n",
    "# file_name = 'fid_1506_tid_97875_pg_4.html'\n",
    "# html_page = load_html_file(dir_path + file_name)\n",
    "# html_posts = get_html_posts(html_page)\n",
    "\n",
    "# post = get_post_info(file_name, html_posts[0])\n",
    "\n",
    "# for key, val in post.items():\n",
    "#     print key + ':', val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### CONVERT TO UTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-28T09:32:13.513459Z",
     "start_time": "2019-01-28T09:32:13.504220Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def convert_to_utc(local_time, local_tz, string=True):\n",
    "    \n",
    "    fmt = '%Y-%m-%d %H:%M:%S %Z%z'\n",
    "    utc = pytz.utc\n",
    "    local_timezone = timezone(local_tz)\n",
    "    \n",
    "    local_datetime = local_timezone.localize(local_time)\n",
    "    utc_datetime = local_datetime.astimezone(utc)\n",
    "    \n",
    "    if string:\n",
    "        utc_datetime = utc_datetime.strftime(fmt)\n",
    "    \n",
    "    return utc_datetime\n",
    "\n",
    "# local_datetime = datetime.strptime('02-17-2018 07:50 AM', '%m-%d-%Y %I:%M %p')\n",
    "# convert_to_utc(local_datetime, 'US/Mountain') # '2018-02-17 14:50:00 UTC+0000'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### USER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### GET USERNAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-28T09:32:13.521427Z",
     "start_time": "2019-01-28T09:32:13.515889Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def get_user_name(el):\n",
    "\n",
    "    user_el = el.find_all('a', {'class': 'bigusername'})[0]\n",
    "    user_name = user_el.get_text()\n",
    "    \n",
    "    return user_name\n",
    "\n",
    "# user_name = get_user_name(trs[1])\n",
    "# print user_name # maytayah"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### GET USER ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-28T09:32:13.529795Z",
     "start_time": "2019-01-28T09:32:13.523706Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def get_user_id(el):\n",
    "\n",
    "    user_el = el.find_all('a', {'class': 'bigusername'})[0]\n",
    "    user_id = user_el['href'].split('=')[-1]\n",
    "    \n",
    "    return user_id\n",
    "\n",
    "# user_id = get_user_id(trs[1])\n",
    "# print user_id # 380798"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### GET USER ROLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-28T09:32:13.548157Z",
     "start_time": "2019-01-28T09:32:13.532531Z"
    },
    "code_folding": [],
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def get_user_role(el):\n",
    "    \n",
    "    user_roles = {'red': 'admin', 'blue': 'super', 'purple': 'site', 'darkgreen': 'mod', \n",
    "                  'magenta': 'leave', 'MediumTurquoise': 'card', 'black': 'user'}\n",
    "    \n",
    "    try:\n",
    "        user_el = el.find_all('a', {'class': 'bigusername'})[0]\n",
    "        # <a class=\"bigusername\" href=\"member.php?u=380798\"><font color=\"purple\"><b>maytayah</b></font></a>\n",
    "        color = user_el.font['color']\n",
    "    \n",
    "    # if user is simply 'registered' then it won't have <font> element\n",
    "    except TypeError as err:\n",
    "        \n",
    "        color = u'black'\n",
    "        \n",
    "    return user_roles[color]\n",
    "\n",
    "# # Administrator: red\n",
    "# # PTO Super Moderator: blue\n",
    "# # PTO Site Moderator: purple\n",
    "# # PTO Moderator: darkgreen\n",
    "# # Moderator On Leave: magenta\n",
    "# # PTO Card Swap Host: MediumTurquoise\n",
    "# # User: black\n",
    "\n",
    "# reg_trs = posts[0].find_all('tr', recursive=False)\n",
    "# mod_trs = posts[6].find_all('tr', recursive=False)\n",
    "\n",
    "# print 'USER ONE:', get_user_role(reg_trs[1]), '\\n', 'USER TWO:', get_user_role(mod_trs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### GET USER ROLE DESCRIPTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-28T09:32:13.556227Z",
     "start_time": "2019-01-28T09:32:13.550811Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def get_user_role_desc(el):\n",
    "    \n",
    "    return el.find_all('div', {'class': 'smallfont'})[0].text\n",
    "\n",
    "# # WILL INCLUDE BANNED AND ACCOUNT CLOSED\n",
    "# reg_trs = posts[0].find_all('tr', recursive=False)\n",
    "# mod_trs = posts[6].find_all('tr', recursive=False)\n",
    "\n",
    "# print 'USER ONE:', get_user_role_desc(reg_trs[1]), '\\n', 'USER TWO:', get_user_role_desc(mod_trs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### GET JOIN DATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-28T09:32:13.566237Z",
     "start_time": "2019-01-28T09:32:13.558713Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def get_join_date(el):\n",
    "    \n",
    "    text = el.find(text=re.compile('Join'))\n",
    "    \n",
    "    if text: \n",
    "        join_date = text.split(':')[1].strip()\n",
    "    else:\n",
    "        join_date = None\n",
    "    \n",
    "    return join_date\n",
    "    \n",
    "# print 'JOIN:', get_join_date(html_posts[30][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### GET LOCATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-28T09:32:13.575968Z",
     "start_time": "2019-01-28T09:32:13.568902Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def get_location(el):\n",
    "    \n",
    "    text = el.find(text=re.compile('Location'))\n",
    "    \n",
    "    if text:\n",
    "        location = text.split(':')[1].strip()\n",
    "    else:\n",
    "        location = None\n",
    "    \n",
    "    return location\n",
    "\n",
    "# print 'LOCATION:', get_location(html_posts[30][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### GET NUM POSTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-28T09:32:13.587351Z",
     "start_time": "2019-01-28T09:32:13.578596Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def get_num_posts(el):\n",
    "    \n",
    "    text = el.find(text=re.compile('Posts'))\n",
    "    \n",
    "    if text:\n",
    "        num_posts = text.split(':')[1].strip()\n",
    "    else:\n",
    "        num_posts = None\n",
    "    \n",
    "    return num_posts\n",
    "\n",
    "# print 'POSTS:', get_num_posts(html_posts[30][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### GET NUM THANKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-28T09:32:13.597533Z",
     "start_time": "2019-01-28T09:32:13.589856Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def get_num_thanks(el):\n",
    "    \n",
    "    text = el.find(text=re.compile('Thanks'))\n",
    "    \n",
    "    if text:\n",
    "        num_thanks = text.split(':')[1].strip()\n",
    "    else:\n",
    "        num_thanks = None\n",
    "        \n",
    "    return num_thanks\n",
    "\n",
    "# print 'THANKS:', get_num_thanks(html_posts[30][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### GET NUM THANKED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-28T09:32:13.615691Z",
     "start_time": "2019-01-28T09:32:13.600259Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def get_num_thanked(el):\n",
    "        \n",
    "    text = el.find(text=re.compile('Thanked'))\n",
    "    \n",
    "    if text:\n",
    "        num_thanked = text.split(' ')[1].strip()\n",
    "    else:\n",
    "        num_thanked = None\n",
    "    \n",
    "    return num_thanked\n",
    "\n",
    "def get_num_posts_thanked(el):\n",
    "    \n",
    "    text = el.find(text=re.compile('Thanked'))\n",
    "    \n",
    "    if text:\n",
    "        num_posts_thanked = text.split(' ')[4].strip()\n",
    "    else:\n",
    "        num_posts_thanked = None\n",
    "    \n",
    "    return num_posts_thanked\n",
    "\n",
    "# print 'THANKED', get_num_thanked(html_posts[30][1]), 'TIMES IN', get_num_posts_thanked(html_posts[30][1]), 'POSTS'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### GET USER INFO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-28T09:32:13.635379Z",
     "start_time": "2019-01-28T09:32:13.618684Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def get_user_info(el):\n",
    "    \n",
    "    user = collections.OrderedDict()\n",
    "    \n",
    "    user['user'] = get_user_name(el[1])\n",
    "    user['uid'] = get_user_id(el[1])\n",
    "    user['role'] = get_user_role(el[1])\n",
    "    user['role_desc'] = get_user_role_desc(el[1])\n",
    "    user['join_date'] = get_join_date(el[1])\n",
    "    user['location'] = get_location(el[1])\n",
    "    user['posts'] = get_num_posts(el[1])\n",
    "    user['thanks'] = get_num_thanks(el[1])\n",
    "    user['thanked'] = get_num_thanked(el[1])\n",
    "    user['posts_thanked'] = get_num_posts_thanked(el[1])\n",
    "    \n",
    "    return user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-28T09:32:13.642800Z",
     "start_time": "2019-01-28T09:32:13.638263Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# html_page = load_html_file(dir_path + '/39/fid_39_tid_81190_pg_10.html')\n",
    "# html_posts = get_html_posts(html_page)\n",
    "# post_nums = [29, 30, 31]\n",
    "\n",
    "# for post_num in post_nums:    \n",
    "    \n",
    "#     user = get_user_info(html_posts[post_num])\n",
    "    \n",
    "#     for key, val in user.items():\n",
    "#         print key + ':', val\n",
    "#     print ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### THANK YOU'S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### GET HTML THANK YOUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-28T09:32:13.650668Z",
     "start_time": "2019-01-28T09:32:13.645832Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def get_html_thank_yous(html_page):\n",
    "        \n",
    "    return html_page.find_all(id=re.compile('post_thanks_box'))\n",
    "    \n",
    "# thanks_none = get_html_thank_yous(html_file_none)\n",
    "# thanks_many = get_html_thank_yous(html_file_many)\n",
    "# len(thanks_none), len(thanks_many)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### GET THANKS LINKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-28T09:32:13.658523Z",
     "start_time": "2019-01-28T09:32:13.653316Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def get_thanks_links(el):\n",
    "    \n",
    "    return el.find_all('a')\n",
    "\n",
    "# no_links = [get_thanks_links(thanks) for thanks in thanks_none]\n",
    "# links = [get_thanks_links(thanks) for thanks in thanks_many]\n",
    "\n",
    "# len(no_links), len(links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### GET THANKED POST ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-28T09:32:13.667814Z",
     "start_time": "2019-01-28T09:32:13.661462Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def get_thanked_post_id(el):\n",
    "    \n",
    "    return el['id'].split('_')[-1]\n",
    "\n",
    "# pid_none = get_thanked_post_id(thanks_none[0])\n",
    "# pid_many = get_thanked_post_id(thanks_many[0])\n",
    "# pid_none, pid_many"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### GET DATE OF THANKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-28T09:32:13.677765Z",
     "start_time": "2019-01-28T09:32:13.670284Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def get_date_of_thanks(el):\n",
    "        # <a>...</a>(02-27-2015),</div>        \n",
    "    ascii_date = el.next_sibling\n",
    "    # u'\\xa0(05-13-2009),\n",
    "    decoded_date = ascii_date.encode('ascii', 'ignore').decode('ascii')\n",
    "    # (05-13-2009),\n",
    "    date_of_thanks = re.sub('[(),]', '', decoded_date)\n",
    "    \n",
    "    return date_of_thanks\n",
    "\n",
    "# get_date_of_thanks(links[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### GET THANKER USER ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-28T09:32:13.696441Z",
     "start_time": "2019-01-28T09:32:13.690929Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def get_thanker_user_id(el):\n",
    "    \n",
    "    return el['href'].split('=')[-1]\n",
    "\n",
    "# get_thanker_user_id(links[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### GET THANKER USER NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-28T09:32:13.705086Z",
     "start_time": "2019-01-28T09:32:13.699107Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def get_thanker_user_name(el):\n",
    "    \n",
    "    return el.text\n",
    "\n",
    "# get_thanker_user_name(links[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### GET THANK YOU INFO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-28T09:32:13.721062Z",
     "start_time": "2019-01-28T09:32:13.707775Z"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def get_thank_you_info(thanks_el, link_el):\n",
    "    \n",
    "    if link_el:\n",
    "    \n",
    "        thank_you = collections.OrderedDict()\n",
    "\n",
    "        thank_you['pid'] = get_thanked_post_id(thanks_el)\n",
    "        thank_you['date'] = get_date_of_thanks(link_el)\n",
    "        thank_you['from_uid'] = get_thanker_user_id(link_el)\n",
    "        thank_you['from_user'] = get_thanker_user_name(link_el)\n",
    "\n",
    "        return thank_you"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## FILE HANDLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-25T14:57:59.039076Z",
     "start_time": "2019-01-25T14:57:59.030537Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# forum_dicts = []\n",
    "\n",
    "# skip = ['logs', 'csv']\n",
    "# directories = get_forum_directories('../examples/threads/parsing/', skip)\n",
    "\n",
    "# # forum_file_dicts = []\n",
    "# # forum_file_dicts = forum_file_dicts + [get_forum_file_dicts(directory, 3) for directory in directories]\n",
    "# # sigh = [file_dict[0] ]\n",
    "# # forum_file_dicts = forum_file_dicts + [get_forum_file_dicts(directory, 3) for directory in directories]\n",
    "    \n",
    "# # forum_dicts = [forum_dict for forum_dict in forum_file_dicts]\n",
    "                             \n",
    "# #                              get_forum_file_dicts(directory, list_size)\n",
    "# #                              for directory in directories]\n",
    "\n",
    "# # forum_dicts\n",
    "\n",
    "\n",
    "# # for forum_file_dict in forum_file_dicts: # for x in non_flat\n",
    "# # #     print forum_file_dict\n",
    "# #     for file_dict in forum_file_dict: # for y in x\n",
    "# #         print len(file_dict['files']) # y\n",
    "\n",
    "# # forum_dicts = [file_dict for forum_file_dict in forum_file_dicts for file_dict in forum_file_dict]\n",
    "\n",
    "# forum_dicts = [file_dict for forum_file_dict in \n",
    "#               [get_forum_file_dicts(directory, 3) for directory in directories] \n",
    "#                for file_dict in forum_file_dict]\n",
    "\n",
    "\n",
    "\n",
    "# forum_dicts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### GET FORUM DIRECTORIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-28T09:32:13.733464Z",
     "start_time": "2019-01-28T09:32:13.726040Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def get_forum_directories(dir_path, skip=[]):\n",
    "    \n",
    "    directories = [str(path) for path in Path(dir_path).iterdir() \n",
    "                   if (path.is_dir() and path.stem not in skip)]\n",
    "    \n",
    "    return directories\n",
    "\n",
    "# skip = ['logs', 'csv']\n",
    "# get_forum_directories('../examples/threads/parsing/', skip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-28T09:32:13.744992Z",
     "start_time": "2019-01-28T09:32:13.737917Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def get_forum_files(forum_path):\n",
    "    \n",
    "    thread_path = Path(forum_path)\n",
    "    all_files = [str(html_file) for html_file in thread_path.rglob('*.html')]\n",
    "    \n",
    "    return all_files\n",
    "\n",
    "\n",
    "# test_path = '../examples/threads/parsing/1506'\n",
    "# get_forum_files(test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### GET FORUM FILE DICTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-28T09:32:13.758490Z",
     "start_time": "2019-01-28T09:32:13.747807Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def get_forum_file_dicts(forum_path, list_size):\n",
    "    \n",
    "    all_files = get_forum_files(forum_path)\n",
    "    \n",
    "    chunks = split_file_list(all_files, 3)\n",
    "    \n",
    "    forum_id = forum_path.split('/')[4]\n",
    "    \n",
    "    forum_file_dicts = [{'fid': forum_id, 'files': chunk} for chunk in chunks]\n",
    "\n",
    "    return forum_file_dicts\n",
    "\n",
    "# htmlpath = '../examples/threads/parsing/1506'\n",
    "# size = 3\n",
    "# forum_file_dicts = get_forum_file_dicts(htmlpath, size)\n",
    "# for forum_dict in forum_file_dicts:\n",
    "#     print forum_dict['fid'] + ':', len(forum_dict['files'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### GET FORUM FILE DICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-28T09:32:13.767159Z",
     "start_time": "2019-01-28T09:32:13.760983Z"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def get_forum_file_dict(dir_path):\n",
    "    \n",
    "    thread_path = Path(dir_path)\n",
    "    forum_files = {'fid': thread_path.stem, 'files': [str(html_file) for html_file in thread_path.rglob('*.html')]}\n",
    "    \n",
    "    return forum_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### LOAD HTML FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-28T09:32:13.782354Z",
     "start_time": "2019-01-28T09:32:13.770148Z"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def load_html_file(file_path):\n",
    "    \n",
    "    html_page = open(file_path, 'r').read()\n",
    "    \n",
    "    posts_page = str(BeautifulSoup(html_page, 'html.parser'))\n",
    "    no_literals = posts_page.replace('\\n', '').replace('\\t', '').replace('\\r', '')\n",
    "    \n",
    "    return BeautifulSoup(no_literals, 'html.parser') \n",
    "    \n",
    "# thread_path = '../examples/parsing/threads/'\n",
    "# forum_id = '412/'\n",
    "# html_file = 'fid_412_tid_102642_pg_1.html'\n",
    "\n",
    "# html_page = load_html_file(thread_path + forum_id + html_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### CREATE FORUM LOGGER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-28T09:32:13.801498Z",
     "start_time": "2019-01-28T09:32:13.785862Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def create_forum_logger(file_path):\n",
    "\n",
    "    log_file = 'PT_' + datetime.strftime(datetime.today(), '%d_%m_%Y_%I_%M_%p') + '.log'\n",
    "    msgfmt = '%(asctime)s %(levelname)s: %(message)s'\n",
    "    datefmt= '%d/%m/%Y %I:%M:%S %p'\n",
    "    \n",
    "    reload(logging)\n",
    "    logging.basicConfig(filename=file_path + '/' + log_file, level=logging.INFO, format=msgfmt, datefmt=datefmt)\n",
    "    logging.Formatter.converter = time.gmtime # CHANGE TO UTC TIME\n",
    "    logger = logging.getLogger()\n",
    "    \n",
    "    return logger\n",
    "\n",
    "# logger = create_forum_logger('../examples/threads/parsing/logs')\n",
    "# html_file = '../examples/threads/parsing/39/fid_39_tid_81190_pg_10.html'.split('/')[-1]\n",
    "\n",
    "# try:\n",
    "#     1/0\n",
    "# except Exception as err:\n",
    "\n",
    "#     logger.error(html_file + ' - ' + err.__class__.__name__ + ': ' + str(err))\n",
    "#     print html_file + ' - ' + err.__class__.__name__ + ': ' + str(err)\n",
    "# # 25/08/2018 01:53:50 AM ERROR: fid_39_tid_81190_pg_10.html - ZeroDivisionError: integer division or modulo by zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### PRINT FINAL RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-28T09:32:13.827578Z",
     "start_time": "2019-01-28T09:32:13.804371Z"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def print_final_results(thread_dfs, thread_data, headers, rows=5):\n",
    "    \n",
    "    for thread_df, data, header in zip(thread_dfs, thread_data, headers):\n",
    "        print 'ROWS:', len(data), 'COLS:', len(header['headers']), 'SHAPE:', thread_df.shape\n",
    "    print ''\n",
    "    \n",
    "    if rows:\n",
    "\n",
    "        print thread_dfs[0][['fid', 'tid', 'pg', 'pid']].head(rows)\n",
    "    #     print thread_dfs[0][['fid', 'tid', 'pg', 'pid']].tail(rows)\n",
    "\n",
    "        print ''\n",
    "        print thread_dfs[1][['user','join_date', 'location', 'posts']].head(rows)\n",
    "    #     print thread_dfs[1][['user','join_date', 'location', 'posts']].tail(rows)\n",
    "\n",
    "        print ''\n",
    "        print thread_dfs[2][['date', 'pid', 'from_uid', 'from_user']].head(rows)\n",
    "    #     print thread_dfs[2][['date', 'pid', 'from_uid', 'from_user']].tail(rows)\n",
    "        print ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### CREATE CSV FILENAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-28T13:10:54.575234Z",
     "start_time": "2019-01-28T13:10:54.566142Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def create_csv_file_path(html_file_path, out_path, data_type):\n",
    "    # '../examples/threads/parsing/1506/ --> fid_1506_tid_97875_pg_4 - .html' + '_type.csv = fid_1506_tid_97875_pg_4_posts.csv\n",
    "    file_name = html_file_path.split('/')[-1][:-5] + ('_' + data_type + '.csv')\n",
    "    csv_file_path = out_path + 'pages/' + data_type + '/' + file_name\n",
    "    \n",
    "    return csv_file_path\n",
    "\n",
    "# hfp = '../data/forums/1506/fid_1506_tid_97875_pg_4.html'\n",
    "# op = '../data/csv/'\n",
    "\n",
    "# hfp = '../examples/threads/parsing/forums/39/fid_39_tid_81190_pg_2.html'\n",
    "# op = '../examples/threads/parsing/csv/'\n",
    "# create_csv_file_path(hfp, op, 'posts') # '../examples/threads/parsing/csv/pages/posts/fid_39_tid_81190_pg_2_posts.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### WRITE PAGE DATA TO CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-28T09:32:13.856910Z",
     "start_time": "2019-01-28T09:32:13.842427Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# MULTIPROCESSING VERSION\n",
    "def write_page_data_to_csv(thread_data, headers, html_file_path, out_path):\n",
    "    \n",
    "    for data, header in zip(thread_data, headers):\n",
    "\n",
    "        csv_file_path = create_csv_file_path(html_file_path, out_path, header['type'])\n",
    "\n",
    "        df = pd.DataFrame(data, columns=header['headers'])\n",
    "        df.to_csv(csv_file_path, index=False, encoding='utf-8')\n",
    "                    \n",
    "#         print file_path, df.shape\n",
    "\n",
    "# TODO: MAKE TESTS THAT DON'T RELY ON PREVIOUSLY RUN CODE\n",
    "# posts_headers = {'type': 'posts', 'headers': ['date', 'fid', 'tid', 'pg', 'post_num', 'pid', 'user', 'uid', 'text']}\n",
    "# users_headers = {'type': 'users', 'headers': ['user', 'uid', 'role', 'role_desc', 'join_date', 'location', 'posts', 'thanks', 'posts_thanked']}\n",
    "# thanks_headers = {'type': 'thanks', 'headers': ['date', 'pid', 'from_uid', 'from_user']}\n",
    "\n",
    "# thread_data = [posts, users, thank_yous]\n",
    "# headers = [posts_headers, users_headers, thanks_headers]\n",
    "# html_file_path = '../examples/threads/parsing/csv/pages/posts/fid_1506_tid_97875_pg_4_posts.csv'\n",
    "# out_path = '../examples/threads/parsing/'\n",
    "\n",
    "# write_page_data_to_csv(thread_data, headers, forum['fid'], out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### SPLIT LIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-27T19:26:46.664378Z",
     "start_time": "2019-01-27T19:26:46.658729Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/a/43106405/6023530\n",
    "\n",
    "def split_list(full_list, num_lists):\n",
    "    return [array.tolist() for array in np.array_split(full_list, num_lists)]\n",
    "      \n",
    "# first_names = ['Steve', 'Jane', 'Sara', 'Mary','Jack','Bob', 'Bily', 'Boni', 'Chris','Sori', 'Will', 'Won','Li']\n",
    "# split_list(first_names, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## PARSING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### PARSE THREADS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-28T13:11:01.906765Z",
     "start_time": "2019-01-28T13:11:01.866732Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def parse_threads(html_file_path):\n",
    "        \n",
    "    try:\n",
    "        \n",
    "        out_path = '../examples/threads/parsing/csv/'\n",
    "        \n",
    "\n",
    "#         out_path = '../data/csv/'\n",
    "        \n",
    "        posts_headers = {'type': 'posts', 'headers': ['date', 'fid', 'tid', 'pg', 'post_num', 'pid', 'user', 'uid', 'text']}\n",
    "        users_headers = {'type': 'users', 'headers': ['user', 'uid', 'role', 'role_desc', 'join_date', 'location', 'posts', 'thanks', 'posts_thanked']}\n",
    "        thanks_headers = {'type': 'thanks', 'headers': ['date', 'pid', 'from_uid', 'from_user']}\n",
    "\n",
    "        headers = [posts_headers, users_headers, thanks_headers]\n",
    "        \n",
    "        posts = []\n",
    "        users = []\n",
    "        thank_yous = []\n",
    "                \n",
    "        html_page = load_html_file(html_file_path)\n",
    "\n",
    "        html_posts = get_html_posts(html_page)\n",
    "\n",
    "        posts = posts + [get_post_info(html_file_path, html_post) for html_post in html_posts]\n",
    "\n",
    "        users = users + [get_user_info(html_post) for html_post in html_posts]\n",
    "\n",
    "\n",
    "        html_thank_yous = get_html_thank_yous(html_page)\n",
    "\n",
    "        post_thank_you_links = [get_thanks_links(html_thank_you) for html_thank_you in html_thank_yous]\n",
    "\n",
    "        thank_yous = thank_yous + [get_thank_you_info(html_thank_you, thank_you_link) \n",
    "                                    for html_thank_you, thank_you_links in zip(html_thank_yous, post_thank_you_links) \n",
    "                                    for thank_you_link in thank_you_links]\n",
    "                \n",
    "#         print html_file_path.split('/')[-1] + ':', posts_headers['type'] + ':', len(posts), users_headers['type'] + ':', len(users), thanks_headers['type'] + ':', len(thank_yous)\n",
    "        \n",
    "        thread_data = [posts, users, thank_yous]\n",
    "        \n",
    "        write_page_data_to_csv(thread_data, headers, html_file_path, out_path)\n",
    "        \n",
    "        return {'posts': len(posts), 'users': len(users), 'thanks': len(thank_yous)}\n",
    "        \n",
    "    except Exception as err:\n",
    "    \n",
    "        file_name = html_file_path.split('/')[-1]\n",
    "        print file_name + ' - ' + err.__class__.__name__ + ': ' + str(err)\n",
    "        \n",
    "# TODO: RIGHT NOW THIS IS DEPENDANT ON THE HEADERS AND THREAD DATA BEING IN THE SAME ORDER.  SHOULD PROBABLY MAKE BOTH INTO DICTIONARIES\n",
    "# SO THAT A KEY CAN BE USED TO MATCH DATA TYPE TO DATA HEADER."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### RUN PARSER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# TODO's\n",
    "# Figure out how to procure the UTC for 'Today' and 'Yesterday'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-28T14:42:42.198909Z",
     "start_time": "2019-01-28T14:42:42.189807Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import parmap\n",
    "import tqdm\n",
    "from random import sample\n",
    "from collections import Counter\n",
    "\n",
    "processors = 1\n",
    "\n",
    "forums_dir = '../examples/threads/parsing/'\n",
    "skip = ['logs', 'csv', 'error']\n",
    "\n",
    "forum_paths = get_forum_directories(forums_dir, skip)\n",
    "\n",
    "pages_by_forum = {forum_path.split('/')[-1] : [str(html_file) for html_file \n",
    "                                               in Path(forum_path).glob('*.html')] \n",
    "                                               for forum_path in forum_paths}\n",
    "totals = Counter()\n",
    "\n",
    "for forum, html_file_paths in pages_by_forum.items():\n",
    "    \n",
    "    print 'FORUM:', forum, '\\n'\n",
    "    \n",
    "    print len(html_file_paths), 'HTML FILES WILL NOW BE PROCESSED BY', processors, 'PROCESSOR(S)' # SHOULD BE 1679\n",
    "    \n",
    "    processed = parmap.map(parse_threads, html_file_paths, pm_processes=processors, pm_pbar=True)\n",
    "    \n",
    "    print len(processed), 'HTML FILES WERE PROCESSED BY', processors, 'PROCESSOR(S)', '\\n'\n",
    "    \n",
    "    totals += report_totals(processed)\n",
    "    \n",
    "    print ''\n",
    "    \n",
    "print 'TOTAL LENGTH OF CSV\\'s:', '\\t', 'POSTS:', totals['posts'], '\\t', 'USERS', totals['users'], '\\t', 'THANKS:', totals['thanks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-28T14:31:55.889150Z",
     "start_time": "2019-01-28T14:31:55.871561Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "def report_totals(results):\n",
    "    \n",
    "    num_files = len(results)\n",
    "    \n",
    "    total_posts = sum([parsed['posts'] for parsed in results])\n",
    "    total_users = sum([parsed['users'] for parsed in results])\n",
    "    total_thanks = sum([parsed['thanks'] for parsed in results])\n",
    "    \n",
    "    csv_posts_lines = total_posts + num_files\n",
    "    csv_users_lines = total_users + num_files\n",
    "    csv_thanks_lines = total_thanks + num_files\n",
    "    \n",
    "    total_processed = total_posts + total_users + total_thanks\n",
    "    total_csv_lines = csv_posts_lines + csv_users_lines + csv_thanks_lines\n",
    "    \n",
    "    print total_processed, 'PROCESSED --->', '\\t', 'POSTS:', total_posts, '\\t', 'USERS', total_users, '\\t', 'THANKS:', total_thanks\n",
    "    print total_csv_lines, 'CSV LINES --->', '\\t', 'POSTS:', csv_posts_lines, '\\t', 'USERS', csv_users_lines, '\\t', 'THANKS:', csv_thanks_lines\n",
    "    \n",
    "    return Counter({'posts': csv_posts_lines, 'users': csv_users_lines, 'thanks': csv_thanks_lines })\n",
    "    \n",
    "# report_totals(processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-28T06:12:37.784Z"
    },
    "code_folding": [],
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 HTML FILES WILL NOW BE PROCESSED BY 1 PROCESSOR(S)\n"
     ]
    }
   ],
   "source": [
    "# # if __name__ == '__main__':\n",
    "import parmap\n",
    "import tqdm\n",
    "from random import sample\n",
    "\n",
    "processors = 1\n",
    "\n",
    "forums_path = '../examples/threads/parsing/'\n",
    "skip = ['logs', 'csv', 'error']\n",
    "\n",
    "# forums_path = '../data/samples/'\n",
    "# skip = ['restricted', 'logs'] # LOGS HAS 43 FILES, RESTRICTED 2\n",
    "\n",
    "dir_paths = get_forum_directories(forums_path, skip)\n",
    "html_file_paths = [str(html_file) for dir_path in dir_paths for html_file in Path(dir_path).rglob('*.html')]\n",
    "test_files = sample(html_file_paths, 100)\n",
    "\n",
    "# TODO: MAKE PROCESSED LIST ACTUALLY CONTAIN SOMETHING INTSTEAD OF NONE\n",
    "print len(test_files), 'HTML FILES WILL NOW BE PROCESSED BY', processors, 'PROCESSOR(S)' # SHOULD BE 1679\n",
    "\n",
    "processed = parmap.map(parse_threads, test_files, pm_processes=processors, pm_pbar=True)\n",
    "\n",
    "print len(processed), 'HTML FILES WERE PROCESSED BY', processors, 'PROCESSOR(S)', '\\n'\n",
    "# report_totals(processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### COMBINE CSV'S BY DATA TYPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-27T17:12:36.757774Z",
     "start_time": "2019-01-27T17:12:35.755653Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA TYPE: posts\n",
      "NUM FRAMES: 40 SUM OF ROWS IN FRAMES: 1009 NUM ROWS IN CONCAT FRAMES: 1009 \n",
      "\n",
      "DATA TYPE: users\n",
      "NUM FRAMES: 40 SUM OF ROWS IN FRAMES: 1009 NUM ROWS IN CONCAT FRAMES: 1009 \n",
      "\n",
      "DATA TYPE: thanks\n",
      "NUM FRAMES: 40 SUM OF ROWS IN FRAMES: 446 NUM ROWS IN CONCAT FRAMES: 446 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "root_path = '../data/csv/'\n",
    "\n",
    "pages_dir = root_path + 'pages/'\n",
    "master_dir = root_path + 'masters/'\n",
    "\n",
    "data_types = ['posts', 'users', 'thanks']\n",
    "\n",
    "csv_pages = {data_type: [str(csv_page_file) for csv_page_file in Path(pages_dir + data_type).iterdir()] for data_type in data_types }\n",
    "\n",
    "for data_type, files in csv_pages.items():\n",
    "    \n",
    "    print 'DATA TYPE:', data_type\n",
    "    \n",
    "    data_frames = parmap.map(pd.read_csv, files, pm_processes=1, pm_pbar=True)\n",
    "    data_frame = pd.concat(data_frames, ignore_index=True)\n",
    "    \n",
    "    csv_file_path = master_dir + 'prison_talk_' + data_type + '.csv'\n",
    "    data_frame.to_csv(csv_file_path, index=False, encoding='utf-8')\n",
    "    \n",
    "    sum_frame_rows = sum([frame.shape[0] for frame in data_frames])\n",
    "    print 'NUM FRAMES:', len(data_frames), 'SUM OF ROWS IN FRAMES:', sum_frame_rows, 'NUM ROWS IN CONCAT FRAMES:', data_frame.shape[0], '\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### SPLIT ALL POSTS CSV INTO FORUMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-27T17:38:07.197436Z",
     "start_time": "2019-01-27T17:38:07.149144Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(master_dir + 'prison_talk_posts.csv')\n",
    "for fid, group in df.groupby(['fid']):\n",
    "    \n",
    "    forum_csv_file = root_path + 'forums/fid_' + str(fid) + '_posts.csv'\n",
    "    group.to_csv(forum_csv_file, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## SCRATCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-28T19:20:28.803121Z",
     "start_time": "2019-01-28T19:20:28.795602Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# DIFFERENT DATA STRUCTURES FOR LOADING FILES \n",
    "\n",
    "forums_path = '../examples/threads/parsing/'\n",
    "skip = ['logs', 'csv', 'error']\n",
    "dir_paths = get_forum_directories(forums_path, skip)\n",
    "html_file_paths = {dir_path.split('/')[-1] : \\\n",
    "                   [str(html_file) for dir_path in dir_paths for html_file in Path(dir_path).rglob('*.html')]}\n",
    "html_file_paths.keys()\n",
    "\n",
    "post_pages = [{'forum': forum, 'files': [str(csv_page_file) for csv_page_file in Path(post_pages_dir).glob('fid_' + forum + '*')]} for forum in forums]\n",
    "\n",
    "i = 0\n",
    "\n",
    "print 'FORUM:', post_pages[i]['forum']\n",
    "print 'FILES:'\n",
    "post_pages[i]['files']\n",
    "\n",
    "csv_pages = [{'data_type': data_type, \n",
    "              'files': [str(csv_page_file) for csv_page_file in Path(pages_dir + data_type).iterdir()]} for data_type in data_types]\n",
    "\n",
    "# csv_pages.keys()\n",
    "post_pages.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-28T12:48:07.681993Z",
     "start_time": "2019-01-28T12:48:07.677058Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# IF YOU WANT TO MAKE A LARGE DICT {'1506', [file_one, file_two...file_n], ...'33', [file_one...] }\n",
    "forums_dir = '../examples/threads/parsing/'\n",
    "skip = ['logs', 'csv', 'error']\n",
    "\n",
    "forum_paths = get_forum_directories(forums_dir, skip)\n",
    "\n",
    "pages_by_forum = {forum_path.split('/')[-1] : [str(html_file) for html_file \n",
    "                                               in Path(forum_path).glob('*.html')] \n",
    "                                               for forum_path in forum_paths}\n",
    "\n",
    "for forum, pages in pages_by_forum.items():\n",
    "    print 'FORUM:', forum, '\\n'\n",
    "    for page in pages:\n",
    "        print page   \n",
    "    print ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-28T12:42:06.252858Z",
     "start_time": "2019-01-28T12:42:06.246919Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# IF YOU WANT TO MAKE A LIST OF DICTS [{'forum': '1506', 'files': [file_one, file_two...file_n]}]\n",
    "\n",
    "forums_dir = '../examples/threads/parsing/'\n",
    "skip = ['logs', 'csv', 'error']\n",
    "\n",
    "forum_paths = get_forum_directories(forums_dir, skip)\n",
    "\n",
    "pages_by_forum = [{'forum': forum_path.split('/')[-1], \n",
    "                   'files': [str(html_file) for html_file \n",
    "                             in Path(forum_path).glob('*.html')]} \n",
    "                             for forum_path in forum_paths]\n",
    "\n",
    "for forum_dict in pages_by_forum:\n",
    "    print 'FORUM:', forum_dict['forum'], '\\n'\n",
    "    for html_file in forum_dict['files']:\n",
    "        print html_file\n",
    "    print ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-25T19:01:21.110068Z",
     "start_time": "2019-01-25T19:01:21.101388Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# WERE ONE TO COMBINE FILES BY ITERATING OVER FORUMS\n",
    "def combine_csv_files(csv_files):\n",
    "    \n",
    "    dfs = []\n",
    "    \n",
    "    for csv_file in csv_files:\n",
    "\n",
    "        dfs.append(pd.read_csv(csv_file))\n",
    "        \n",
    "    return pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# csv_post_files = ['../examples/threads/parsing/csv/pages/posts/fid_1506_tid_93600_pg_1_posts.csv',\n",
    "#                   '../examples/threads/parsing/csv/pages/posts/fid_1506_tid_93600_pg_2_posts.csv',\n",
    "#                   '../examples/threads/parsing/csv/pages/posts/fid_1506_tid_93600_pg_3_posts.csv']\n",
    "\n",
    "# concat_df = combine_csv_files(csv_post_files)\n",
    "# concat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#OLD VERSION BEFORE MULTIPROCESSING\n",
    "def write_thread_data_to_csv(thread_data, headers, fid, out_path):\n",
    "    \n",
    "    dfs = []\n",
    "    \n",
    "    for data, header in zip(thread_data, headers):\n",
    "\n",
    "        data_type = header['type']\n",
    "\n",
    "        file_name = data_type + '_fid_' + fid + '.csv'\n",
    "        file_path = out_path + 'csv/' + data_type + '/' + file_name\n",
    "\n",
    "        df = pd.DataFrame(data, columns=header['headers'])\n",
    "        df.to_csv(file_path, index=False, encoding='utf-8')\n",
    "        \n",
    "        dfs.append(df)\n",
    "        \n",
    "    return dfs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# OKAY, SO WHAT'S GOING ON HERE IS THAT WHATEVER I PASS TO THE MAP FUNCTION WILL BE AN ITERABLE\n",
    "# AND THE MAP FUNCTION WILL TAKE ONE OF THE ITERABLE.  FOR EXAMPLE\n",
    "\n",
    "# NUMS = [NUM_ZERO, NUM_ONE, NUM_TWO...NUM_N]\n",
    "# NUM_FUNC(NUM)\n",
    "\n",
    "# WOULD BE: POOL.MAP(NUM_FUNC, NUMS)\n",
    "\n",
    "# SO WHAT I NEED TO DO IS PASS IT AN ITERABLE OF HTML FILES BUT MAKE THE FUNCTION SUCH THAT IT TAKES ONE FILE\n",
    "\n",
    "# FORUMS = [{FID: 0, FILES: ['FILE_ONE.TXT', 'FILE_TW0.TXT', 'FILE_THREE.TXT']}]\n",
    "# PARSE_THREADS(HTML_FILE):\n",
    "#     DO STUFF WITH THE HTML_FILE\n",
    "\n",
    "# FOR FORUM IN FORUMS:\n",
    "#     PARSED_THREADS[FORUM['FID']] = POOL.MAP(PARSE_THREADS, FORUM['FILES'])\n",
    "\n",
    "    \n",
    "# HERE THE FORUM['FILES'] IS EQUIVALENT TO THE NUMS\n",
    "# PARSE_THREADS(HTML_FILE) IS EQUIVALENT TO NUM_FUNC(NUM)\n",
    "# AND A SINGLE HTML_FILE IS EQUIVALENT TO ONE NUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my.file - ZeroDivisionError: integer division or modulo by zero\n"
     ]
    }
   ],
   "source": [
    "# EXCEPTION TESTING\n",
    "some_file = '../this/is/my.file'\n",
    "try:\n",
    "    1/0\n",
    "\n",
    "except Exception as err:\n",
    "    \n",
    "    file_name = some_file.split('/')[-1]\n",
    "    print file_name + ' - ' + err.__class__.__name__ + ': ' + str(err)\n",
    "#     tb = sys.exc_info()[-1]\n",
    "#     stk = traceback.extract_tb(tb, 1)\n",
    "#     function_name = stk[0][3]\n",
    "#     print 'The failing function was', function_name\n",
    "\n",
    "#     log_msg = file_name + ' - ' + function_name + ' - ' + err.__class__.__name__ + ': ' + str(err)\n",
    "\n",
    "#     logger.error(log_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-25T14:58:03.623553Z",
     "start_time": "2019-01-25T14:58:03.601086Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#OLD VERSION BEFORE MULTIPROCESSING\n",
    "def write_thread_data_to_csv(thread_data, headers, fid, out_path):\n",
    "    \n",
    "    dfs = []\n",
    "    \n",
    "    for data, header in zip(thread_data, headers):\n",
    "\n",
    "        data_type = header['type']\n",
    "\n",
    "        file_name = data_type + '_fid_' + fid + '.csv'\n",
    "        file_path = out_path + 'csv/' + data_type + '/' + file_name\n",
    "\n",
    "        df = pd.DataFrame(data, columns=header['headers'])\n",
    "        df.to_csv(file_path, index=False, encoding='utf-8')\n",
    "        \n",
    "        dfs.append(df)\n",
    "        \n",
    "    return dfs\n",
    "            \n",
    "#         print file_path, df.shape\n",
    "        \n",
    "# posts_headers = {'type': 'posts', 'headers': ['date', 'fid', 'tid', 'pg', 'post_num', 'pid', 'user', 'uid', 'text']}\n",
    "# users_headers = {'type': 'users', 'headers': ['user', 'uid', 'role', 'role_desc', 'join_date', 'location', 'posts', 'thanks', 'posts_thanked']}\n",
    "# thanks_headers = {'type': 'thanks', 'headers': ['date', 'pid', 'from_uid', 'from_user']}\n",
    "\n",
    "# thread_data = [posts, users, thank_yous]\n",
    "# headers = [posts_headers, users_headers, thanks_headers]\n",
    "# out_path = '../examples/threads/parsing/'\n",
    "\n",
    "# write_thread_data_to_csv(thread_data, headers, forum['fid'], out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# if __name__ == '__main__':\n",
    "import parmap\n",
    "import tqdm\n",
    "\n",
    "forums_path = '../examples/threads/parsing/'\n",
    "skip = ['logs', 'csv', 'error']\n",
    "\n",
    "\n",
    "\n",
    "# directory_paths = get_forum_directories(forums_path, skip)\n",
    "# forums = [get_forum_file_dict(directory_path) for directory_path in directory_paths]\n",
    "\n",
    "dir_paths = get_forum_directories(forums_path, skip)\n",
    "html_file_paths = [str(html_file) for dir_path in dir_paths for html_file in Path(dir_path).rglob('*.html')]\n",
    "\n",
    "parmap.map(parse_threads, html_file_paths, pm_processes=1, pm_pbar=True)\n",
    "\n",
    "\n",
    "# parsed_threads = {}\n",
    "\n",
    "# pool = mp.Pool(processes=1)\n",
    "# pool.map(target=parse_threads, args(html_files)\n",
    "\n",
    "# for forum in forums:\n",
    "#     fid = forum['fid']\n",
    "#     parsed_threads[forum['fid']] = pool.map(parse_threads, forum['files'])\n",
    "\n",
    "# pool.close()\n",
    "# pool.join()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Initialization Cell",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "154px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {
    "height": "933px",
    "left": "0px",
    "right": "1371.78px",
    "top": "108px",
    "width": "282px"
   },
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
